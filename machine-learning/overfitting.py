# ì˜¤ë²„í”¼íŒ…ê³¼ ë¦¿ì§€, ë¼ì˜ ëª¨ë¸ ì´í•´í•˜ê¸°
# ì˜¤ë²„í”¼íŒ…(Overfitting) : ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì— ë„ˆë¬´ ì˜ ë§ì¶°ì ¸ì„œ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” í˜„ìƒ
# R2 ê°’ì´ ë†’ë‹¤í•´ì„œ ê·¸ì € ì¢‹ì€ê²Œ ì•„ë‹ˆë¼ëŠ” ëœ».

# ë¦¿ì§€(Ridge) : L2 ì •ê·œí™” ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì¤„ì´ëŠ” ë°©ë²•
# ë¼ì˜(Lasso) : L1 ì •ê·œí™” ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì¤„ì´ëŠ” ë°©ë²•
# ë¦¿ì§€ì™€ ë¼ì˜ëŠ” ì˜¤ë²„í”¼íŒ…ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ì •ê·œí™” ê¸°ë²•ì…ë‹ˆë‹¤.
                # ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì— íŒ¨ë„í‹°ë¥¼ ë¶€ì—¬í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.

# ë‹¤í•­íšŒê·€ëª¨í˜•
# ğœ™j(x) = x^j

import numpy as np
import pandas as pd
np.random.seed(2021)
x = np.random.choice(np.arange(0, 1.05, 0.05), size=10, replace=False)
y = np.sin(2 * np.pi * x) + np.random.normal(0, 0.2, len(x))
x2 = np.linspace(0, 1, 100)
y2 = (2 * x + 3)
mydata = pd.DataFrame({'x': x, 'y': y})
mydata = mydata.sort_values('x').reset_index(drop=True)
print(mydata)


# ë‹¤í•­ íšŒê·€ë¶„ì„ ì˜ˆì‹œ ì‹œê°í™”
import matplotlib.pyplot as plt
x2 = np.linspace(0, 1, 100)
y2 = np.sin(2 * np.pi * x2)

plt.figure(figsize=(6, 4))
plt.scatter(mydata['x'], mydata['y'],
color='black', label='Observed')
plt.plot(x2, y2, color='red',
label='True Curve')
plt.title('Data and True Curve')
plt.legend()
plt.grid(True)
plt.show()


# 0ì°¨ ë‹¤í•­ íšŒê·€ì‹ (ì ˆí¸ë§Œ ì‚¬ìš©í•œ ëª¨ë¸)
plt.figure(figsize=(6, 4))
plt.scatter(mydata['x'], mydata['y'],
color='black', label='Observed')
plt.plot(x2, y2, color='red',
label='True Curve')
plt.axhline(y=np.mean(mydata['y']),
color='blue',
label='Mean Model')
plt.title('0-degree Polynomial Regression')
plt.legend()
plt.grid(True)
plt.show()



# 1ì°¨ ë‹¤í•­ íšŒê·€ì‹ (ì„ í˜• íšŒê·€ ëª¨ë¸)
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
poly1 = PolynomialFeatures(degree=1, include_bias=True)

x2 = np.linspace(0, 1, 100)
y2 = np.sin(2 * np.pi * x2)

X1 = poly1.fit_transform(mydata[['x']])
X1.shape
model1 = LinearRegression().fit(X1, mydata['y'])
y1_pred = model1.predict(poly1.transform(x2.reshape(-1, 1)))
                                         
plt.figure(figsize=(6, 4))
plt.scatter(mydata['x'], mydata['y'], color='black', label='Observed')
plt.plot(x2, y2, color='red', label='True Curve')
plt.plot(x2, y1_pred, color='blue', label='Degree 1')
plt.title('1-degree Polynomial Regression')
plt.legend()
plt.grid(True)
plt.show()




# 3ì°¨ ë‹¤í•­ íšŒê·€ì‹
poly2 = PolynomialFeatures(degree=3, include_bias=True)
X2 = poly2.fit_transform(mydata[['x']])
model2 = LinearRegression().fit(X2, mydata['y'])
y2_pred = model2.predict(poly2.transform(x2.reshape(-1, 1)))

plt.figure(figsize=(6, 4))
plt.scatter(mydata['x'], mydata['y'], color='black', label='Observed')
plt.plot(x2, y2, color='red', label='True Curve')
plt.plot(x2, y2_pred, color='blue', label='Degree 3 Fit')
plt.title('2-degree Polynomial Regression')
plt.legend()
plt.grid(True)
plt.show()




# 4ì°¨ ë‹¤í•­ íšŒê·€ì‹
poly4 = PolynomialFeatures(degree=4, include_bias=True)
X2 = poly4.fit_transform(mydata[['x']])
model2 = LinearRegression().fit(X2, mydata['y'])
y2_pred = model2.predict(poly4.transform(x2.reshape(-1, 1)))

plt.figure(figsize=(6, 4))
plt.scatter(mydata['x'], mydata['y'], color='black', label='Observed')
plt.plot(x2, y2, color='red', label='True Curve')
plt.plot(x2, y2_pred, color='blue', label='Degree 3 Fit')
plt.title('2-degree Polynomial Regression')
plt.legend()
plt.grid(True)
plt.show()



# 1~9 ì°¨ ë‹¤í•­íšŒê·€ì‹
for i in range(1, 10):
    poly_i = PolynomialFeatures(degree=i, include_bias=True)
    X2 = poly_i.fit_transform(mydata[['x']])
    model2 = LinearRegression().fit(X2, mydata['y'])
    y2_pred = model2.predict(poly_i.transform(x2.reshape(-1, 1)))

    plt.figure(figsize=(6, 4))
    plt.scatter(mydata['x'], mydata['y'], color='black', label='Observed')
    plt.plot(x2, y2, color='red', label='True Curve')
    plt.plot(x2, y2_pred, color='blue', label=f'Degree {i} Fit')
    plt.title(f'{i}-degree Polynomial Regression')
    plt.ylim(-2, 2)
    plt.legend()
    plt.grid(True)
    plt.show()


'''
Overfittingì˜ ë¬¸ì œì 
1. 9ì°¨ ë‹¤í•­ íšŒê·€ëª¨ë¸ ì‚¬ìš©ì‹œ ë¬¸ì œì 
- íšŒê·€ë¶„ì„ì—ì„œì˜ Rules of thumbì€ 10ê°œ í‘œë³¸ë‹¹ 1ê°œ ë…ë¦½ë³€ìˆ˜!

2. ë¶„ì„ ë° ì˜ˆì¸¡ì—ì„œì˜ ë¬¸ì œì 
- ê´€ë ¨ ì—†ëŠ” ë³€ìˆ˜ë“¤ì´ ì±„íƒë¨
- í•™ìŠµ ë°ì´í„°ì—ì„œ ì˜ˆì¸¡ë ¥ì´ ì¢‹ì§€ë§Œ ë™ì¼í•œ ë°ì´í„° ë°œìƒ ëª¨ë¸ì—ì„œì˜ ê´€ì¸¡ê°’ì— ëŒ€í•œ ì˜ˆì¸¡ë ¥ì€ í˜„ì €
íˆ ë–¨ì–´ì§€ê²Œ ë¨. (modelì˜ varianceê°€ ì¦ê°€!)
'''
# train set
np.random.seed(2021)
x = np.random.choice(np.arange(0, 1.05, 0.05), size=100, replace=True)
y = np.sin(2 * np.pi * x) + np.random.normal(0, 0.2, len(x))
data_learning = pd.DataFrame({'x': x, 'y': y})



# validation set
x_test = np.random.choice(np.arange(0, 1.05, 0.05), size=5, replace=True)
true_test_y = np.sin(2 * np.pi * x_test) + np.random.normal(0, 0.2, len(x_test))
data_test = pd.DataFrame({'x': x_test, 'y': [np.nan] * len(x_test)})


# train set -> train, vaild ë‚˜ëˆ„ê¸°
from sklearn.model_selection import train_test_split
# 7: 3 ë¹„ìœ¨ë¡œ ë‚˜ëˆ„ê¸°
train, valid = train_test_split(data_learning, test_size=0.3, random_state=1234)


# train ì…‹ vs. validation ì…‹ ëª¨ë¸ ì„±ëŠ¥ë¹„êµ
# ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ëŠ˜ë ¤ê°€ë©´ì„œ ë°ì´í„°ë¥¼ ì í•©í•˜ê³ , valid ì…‹ì—ì„œì˜ ì„±ëŠ¥ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

perform_train = []
perform_valid = []

for i in range(1, 21):
    poly = PolynomialFeatures(degree=i, include_bias=True)
    X_train = poly.fit_transform(train[['x']])
    X_valid = poly.transform(valid[['x']])
    model = LinearRegression().fit(X_train, train['y'])
    y_train_pred = model.predict(X_train)
    y_valid_pred = model.predict(X_valid)
    mse_train = mean_squared_error(train['y'], y_train_pred)
    mse_valid = mean_squared_error(valid['y'], y_valid_pred)
    perform_train.append(mse_train)
    perform_valid.append(mse_valid)
best_degree = np.argmin(perform_valid) + 1
print("Best polynomial degree:", best_degree)



# ëª¨ë¸ ì„±ëŠ¥ë¹„êµ ì‹œê°í™”
# ë¹¨ê°„ìƒ‰ ì„ ì„ ìµœì†Œë¡œ ë§Œë“œëŠ” ëª¨ë¸ì„ ì°¾ëŠ” ê²ƒì´ ëª©í‘œ
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.plot(range(1, 21), perform_train, label="Train MSE")
plt.plot(range(1, 21), perform_valid, label="Valid MSE", color='red')
plt.xlabel("Polynomial Degree")
plt.ylabel("Mean Squared Error")
plt.title("Model Complexity vs MSE")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()





# 8ì¡°
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams['font.family'] ='Malgun Gothic'
plt.rcParams['axes.unicode_minus'] =False


# ëœë¤ ì‹œë“œë¥¼ ê³ ì •í•˜ì—¬ ì‹¤í–‰í•  ë•Œë§ˆë‹¤ ë™ì¼í•œ ê²°ê³¼ê°€ ë‚˜ì˜¤ë„ë¡ ì„¤ì •
np.random.seed(2021)

# 0ë¶€í„° 1ê¹Œì§€ 0.05 ê°„ê²©ìœ¼ë¡œ ê°’ì„ ì„ íƒí•´ 40ê°œì˜ ìƒ˜í”Œì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒ (ì¤‘ë³µ í—ˆìš©)
x = np.random.choice(np.arange(0, 1.05, 0.05), size=40, replace=True)

# ì„ íƒëœ xê°’ì— ëŒ€í•´ sin í•¨ìˆ˜ ì ìš©í•˜ê³ , í‰ê·  0, í‘œì¤€í¸ì°¨ 0.2ì¸ ì •ê·œë¶„í¬ ë…¸ì´ì¦ˆ ì¶”ê°€
y = np.sin(2 * np.pi * x) + np.random.normal(0, 0.2, len(x))

# xì™€ yë¥¼ í•©ì³ì„œ í•™ìŠµìš© ë°ì´í„°í”„ë ˆì„ ìƒì„±
data_for_learning = pd.DataFrame({'x': x, 'y': y})

# í•™ìŠµìš© ë°ì´í„°ì…‹ì„ í•™ìŠµ(train)ê³¼ ê²€ì¦(valid) ì„¸íŠ¸ë¡œ 7:3 ë¹„ìœ¨ë¡œ ë¶„í• 
from sklearn.model_selection import train_test_split
train, valid = train_test_split(data_for_learning, test_size=0.3, random_state=1234)

# ì‚¬ìš©í•  ëª¨ë“ˆ import
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

i = 3  # ë‹¤í•­ì‹ì˜ ì°¨ìˆ˜ë¥¼ ì„¤ì • (ex. 3ì°¨ ë‹¤í•­ íšŒê·€)

# 0~1 ë²”ìœ„ì—ì„œ 100ê°œì˜ ê· ì¼í•œ ê°’ì„ ê°–ëŠ” ì„ í˜• ë°°ì—´ ìƒì„± (ëª¨ë¸ ê³¡ì„ ì„ ê·¸ë¦´ ë•Œ ì‚¬ìš©)
k = np.linspace(0, 1, 100)

# ì°¸ê°’ í•¨ìˆ˜: y = sin(2Ï€x)
sin_k = np.sin(2 * np.pi * k)

# ì°¨ìˆ˜ê°€ iì¸ ë‹¤í•­ íŠ¹ì„± ìƒì„±ê¸° ì •ì˜ (include_bias=Trueë©´ ì ˆí¸ í•­ í¬í•¨ë¨)
poly1 = PolynomialFeatures(degree=i, include_bias=True)

# í•™ìŠµ ë°ì´í„° xë¥¼ ë‹¤í•­ íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜ (1, x, x^2, ..., x^i)
train_X = poly1.fit_transform(train[['x']])

# ì„ í˜• íšŒê·€ ëª¨ë¸ í•™ìŠµ
model1 = LinearRegression().fit(train_X, train['y'])

# ì˜ˆì¸¡ ê³¡ì„ : k ê°’ì„ ë‹¤í•­ íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜í•œ í›„ ì˜ˆì¸¡ê°’ ê³„ì‚°
model_line_blue = model1.predict(poly1.transform(k.reshape(-1, 1)))

# í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’
train_y_pred = model1.predict(poly1.transform(train[['x']]))

# ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’
valid_y_pred = model1.predict(poly1.transform(valid[['x']]))

# í•™ìŠµ ë°ì´í„°ì˜ í‰ê· ì œê³±ì˜¤ì°¨ (MSE)
mse_train = mean_squared_error(train['y'], train_y_pred)

# ê²€ì¦ ë°ì´í„°ì˜ í‰ê· ì œê³±ì˜¤ì°¨ (MSE)
mse_valid = mean_squared_error(valid['y'], valid_y_pred)

# ì‹œê°í™”ë¥¼ ìœ„í•œ subplot êµ¬ì„± (1í–‰ 2ì—´)
fig, axes = plt.subplots(1, 2, figsize=(12, 4))

# ì™¼ìª½ ê·¸ë˜í”„: í•™ìŠµ ë°ì´í„° ì‹œê°í™”
axes[0].scatter(train['x'], train['y'], color='black', label='Train Observed')  # ì‹¤ì œ í•™ìŠµ ë°ì´í„° ì 
axes[0].plot(k, sin_k, color='red', alpha=0.1, label='True Curve')  # ì°¸ í•¨ìˆ˜ (ì–‡ì€ ë¹¨ê°„ ì„ )
axes[0].plot(k, model_line_blue, color='blue', label=f'Degree {i} Fit')  # ëª¨ë¸ ì˜ˆì¸¡ ê³¡ì„  (íŒŒë€ ì„ )
axes[0].text(0.05, -1.8, f'MSE: {mse_train:.4f}', fontsize=10, color='blue')  # í•™ìŠµ MSE í…ìŠ¤íŠ¸ í‘œì‹œ
axes[0].set_title(f'{i}-degree Polynomial Regression (Train)')  # ê·¸ë˜í”„ ì œëª©
axes[0].set_ylim((-2.0, 2.0))  # yì¶• ë²”ìœ„ ì„¤ì •
axes[0].legend()  # ë²”ë¡€ í‘œì‹œ
axes[0].grid(True)  # ê·¸ë¦¬ë“œ í‘œì‹œ

# ì˜¤ë¥¸ìª½ ê·¸ë˜í”„: ê²€ì¦ ë°ì´í„° ì‹œê°í™”
axes[1].scatter(valid['x'], valid['y'], color='green', label='Valid Observed')  # ì‹¤ì œ ê²€ì¦ ë°ì´í„° ì 
axes[1].plot(k, sin_k, color='red', alpha=0.1, label='True Curve')  # ì°¸ í•¨ìˆ˜
axes[1].plot(k, model_line_blue, color='blue', label=f'Degree {i} Fit')  # ëª¨ë¸ ì˜ˆì¸¡ ê³¡ì„ 
axes[1].text(0.05, -1.8, f'MSE: {mse_valid:.4f}', fontsize=10, color='blue')  # ê²€ì¦ MSE í…ìŠ¤íŠ¸ í‘œì‹œ
axes[1].set_title(f'{i}-degree Polynomial Regression (Valid)')  # ê·¸ë˜í”„ ì œëª©
axes[1].set_ylim((-2.0, 2.0))  # yì¶• ë²”ìœ„ ì„¤ì •
axes[1].legend()  # ë²”ë¡€ í‘œì‹œ
axes[1].grid(True)  # ê·¸ë¦¬ë“œ í‘œì‹œ

# ì „ì²´ ë ˆì´ì•„ì›ƒ ì •ë ¬
plt.tight_layout()

# ê·¸ë˜í”„ í‘œì‹œ
plt.show()






import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# ë°ì´í„° ìƒì„±
np.random.seed(2021)
x = np.random.choice(np.arange(0, 1.05, 0.05), size=40, replace=True)
y = np.sin(2 * np.pi * x) + np.random.normal(0, 0.2, len(x))
data = pd.DataFrame({'x': x, 'y': y})

# train/valid ë‚˜ëˆ„ê¸°
train, valid = train_test_split(data, test_size=0.3, random_state=1234)

# ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
degrees = range(1, 16)
train_mse_list = []
valid_mse_list = []

# ì°¨ìˆ˜ë³„ë¡œ ë°˜ë³µ
for i in degrees:
    # ë‹¤í•­ íŠ¹ì„± ë³€í™˜ê¸° ì •ì˜
    poly = PolynomialFeatures(degree=i, include_bias=True)

    # ë°ì´í„° ë³€í™˜
    X_train_poly = poly.fit_transform(train[['x']])
    X_valid_poly = poly.transform(valid[['x']])

    # íšŒê·€ ëª¨ë¸ í•™ìŠµ
    model = LinearRegression().fit(X_train_poly, train['y'])

    # ì˜ˆì¸¡
    y_train_pred = model.predict(X_train_poly)
    y_valid_pred = model.predict(X_valid_poly)

    # MSE ê³„ì‚°
    train_mse = mean_squared_error(train['y'], y_train_pred)
    valid_mse = mean_squared_error(valid['y'], y_valid_pred)

    train_mse_list.append(train_mse)
    valid_mse_list.append(valid_mse)

# ìµœì  ì°¨ìˆ˜ êµ¬í•˜ê¸° (valid MSEê°€ ìµœì†Œì¸ ê²½ìš°)
best_degree = degrees[np.argmin(valid_mse_list)]
best_mse = min(valid_mse_list)

print(f"ìµœì  ì°¨ìˆ˜: {best_degree}ì°¨, Valid MSE: {best_mse:.4f}")

# ì‹œê°í™”
plt.figure(figsize=(10, 5))
plt.plot(degrees, train_mse_list, marker='o', label='Train MSE')
plt.plot(degrees, valid_mse_list, marker='s', label='Valid MSE')
plt.axvline(best_degree, color='red', linestyle='--', label=f'Best Degree = {best_degree}')
plt.title('ë‹¤í•­ íšŒê·€ ì°¨ìˆ˜ë³„ MSE ë¹„êµ')
plt.xlabel('ë‹¤í•­ì‹ ì°¨ìˆ˜ (Degree)')
plt.ylabel('MSE')
plt.ylim(-0.2, 1)
plt.xlim(0, 10)
plt.xticks(degrees)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()



# bias ë€?
# ë‚´ê°€ ì˜ˆì¸¡í•œ ê°’ê³¼ ì‹¤ì œ ê°’ì˜ ì°¨ì´
# biasê°€ í¬ë©´ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ê°€ í¬ë‹¤ëŠ” ëœ»
# biasê°€ ì‘ìœ¼ë©´ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ê°€ ì‘ë‹¤ëŠ” ëœ»
# biasê°€ í¬ë©´ ì˜¤ë²„í”¼íŒ…ì´ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.

# bias ì™€ variance ì˜ ê´€ê³„
# varianceëŠ” ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì— ì–¼ë§ˆë‚˜ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ
# biasê°€ í¬ë©´ varianceê°€ ì‘ê³ , biasê°€ ì‘ìœ¼ë©´ varianceê°€ í¬ë‹¤.
# biasì™€ varianceëŠ” ì„œë¡œ trade-off ê´€ê³„ì— ìˆë‹¤.




# ìš°ë¦¬ê°€ ëœë¤ìœ¼ë¡œ ë°ì´í„°ì…‹ê³¼ validation ì…‹ì„ ë‚˜ëˆ„ì—ˆê¸° ë•Œë¬¸ì—
# validation ì…‹ì—ì„œì˜ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.
# ë”°ë¼ì„œ, validation ì…‹ì—ì„œì˜ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ì„œëŠ”
# í…ŒìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ë²ˆ ë°˜ë³µí•´ì„œ í‰ê· ì„ ë‚´ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.
# k-fold cross-validationì„ ì‚¬ìš©
# ë°ì´í„°ë¥¼ kê°œë¡œ ë‚˜ëˆ„ì–´ì„œ k-1ê°œë¡œ í•™ìŠµí•˜ê³  1ê°œë¡œ ê²€ì¦í•˜ëŠ” ë°©ë²•ì´ë‹¤.
# cross-validationì€ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ë²ˆ ë‚˜ëˆ„ì–´ì„œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ê²€ì¦í•˜ëŠ” ë°©ë²•ì´ë‹¤.
# cross-validationì„ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë” ì •í™•í•˜ê²Œ í‰ê°€í•  ìˆ˜ ìˆë‹¤.

# íŒ¨ë„í‹°ë¥¼ ì´ìš©í•œ ëª¨ë¸ì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤.
# íŒ¨ë„í‹°ë¥¼ ì´ìš©í•œ ëª¨ë¸ì€ ë¦¿ì§€ íšŒê·€ì™€ ë¼ì˜ íšŒê·€ê°€ ìˆë‹¤.


# ë²¡í„° Norm ì˜ë¯¸
# ì˜ˆì‹œ: (2, -3, 7) ë²¡í„°ì˜ L1 Norm
# L1 Norm = |2| + |-3| + |7| = 2 + 3 + 7 = 12
# L2 Normì€ ë²¡í„°ì˜ ê° ì„±ë¶„ì„ ì œê³±í•œ í›„ ë”í•œ ê°’ì˜ ì œê³±ê·¼
# L2 Norm = âˆš(2^2 + (-3)^2 + 7^2) = âˆš(4 + 9 + 49) = âˆš62 â‰ˆ 7.87

# Lâˆ Norm = max(|2|, |-3|, |7|) = 7
# L1 Normì€ ë²¡í„°ì˜ ê° ì„±ë¶„ì˜ ì ˆëŒ“ê°’ì„ ë”í•œ ê°’

# ë¼ì˜ íšŒê·€ì§ì„ 
# íŒ¨ë„í‹°í•­ ì¶”ê°€
# L1 Normì„ ì‚¬ìš©í•˜ì—¬ íŒ¨ë„í‹°ë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ë²•

# ëŒë‹¤ê°€ 0ì¼ ë•ŒëŠ” ì¼ë°˜ì ì¸ ì„ í˜• íšŒê·€ì™€ ê°™ê³ ,
# ëŒë‹¤ê°€ ì»¤ì§ˆìˆ˜ë¡ íŒ¨ë„í‹°ê°€ ì»¤ì ¸ì„œ íšŒê·€ê³„ìˆ˜ê°€ 0ì— ê°€ê¹Œì›Œì§„ë‹¤.
# ë”°ë¼ì„œ, ëŒë‹¤ê°€ ì»¤ì§ˆìˆ˜ë¡ ëª¨ë¸ì´ ë‹¨ìˆœí•´ì§„ë‹¤.

# ì¦‰, ëŒë‹¤ê°€ ì»¤ì§ˆìˆ˜ë¡ ëª¨ë¸ì´ ë‹¨ìˆœí•´ì§€ë©´ì„œ ì˜¤ë²„í”¼íŒ…ì„ ë°©ì§€í•  ìˆ˜ ìˆë‹¤.


# ë¦¿ì§€: ë‹¤ì¤‘ê³µì„ ì„±ì´ ìˆëŠ”ê²½ìš° ì•ˆì •ì ì´ë‹¤.


# ë¼ì˜íšŒê·€  ì–´ë–»ê²Œêµ¬í˜„í•˜ë‚˜ìš”
# scikit-learnì˜ Lasso í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¼ì˜ íšŒê·€ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


# ì˜ˆì œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
import pandas as pd
from sklearn.datasets import load_iris

iris = load_iris()
iris = pd.DataFrame(iris.data, columns=iris.feature_names)
iris.columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width']

X = iris[['Petal_Width', 'Sepal_Length', 'Sepal_Width']]
# X = np.column_stack((np.ones(len(X)), X)) # ì ˆí¸í•­ ì¶”ê°€
y = iris['Petal_Length'].values


# ë¼ì˜ ëª¨ë¸ ì í•©í•˜ê¸°
from sklearn.linear_model import Lasso
lasso_model = Lasso(alpha=10)  # alphaëŠ” íŒ¨ë„í‹° í•­ì˜ ê³„ìˆ˜ (ëŒë‹¤)
lasso_model.fit(X, y)
lasso_model.coef_  # íšŒê·€ê³„ìˆ˜
lasso_model.intercept_  # ì ˆí¸

lasso_model = Lasso(alpha=0)  # alphaëŠ” íŒ¨ë„í‹° í•­ì˜ ê³„ìˆ˜ (ëŒë‹¤)
lasso_model.fit(X, y)
lasso_model.coef_  # íšŒê·€ê³„ìˆ˜
lasso_model.intercept_  # ì ˆí¸
# ëŒë‹¤ ê°’ì´ ì»¤ì§€ë©´ íšŒê·€ê³„ìˆ˜ê°€ 0ì— ê°€ê¹Œì›Œì§„ë‹¤.
# ëŒë‹¤ ê°’ì´ ì‘ì•„ì§€ë©´ íšŒê·€ê³„ìˆ˜ê°€ ì»¤ì§„ë‹¤.
# ëŒë‹¤ë¥¼ í¬ê²Œ ì„¤ì •í•˜ë©´ ì£½ëŠ” ë³€ìˆ˜ ê°œìˆ˜ê°€ ë§ì•„ì§„ë‹¤.
# ëŒë‹¤ê°€ ì‘ìœ¼ë©´ ë³€ìˆ˜ë¥¼ ë§ì´ ì‚¬ìš©í•˜ê²Œ ëœë‹¤.


# ë¦¿ì§€ë¡œ ë°”ê¾¸ì
from sklearn.linear_model import Ridge
ridge_model = Ridge(alpha=1.1)  # alphaëŠ” íŒ¨ë„í‹° í•­ì˜ ê³„ìˆ˜ (ëŒë‹¤)
ridge_model.fit(X, y)
ridge_model.coef_  # íšŒê·€ê³„ìˆ˜
ridge_model.intercept_  # ì ˆí¸

# ë¦¿ì§€íšŒê·€ëŠ” ëŒë‹¤ê°€ ì»¤ë„ ê³„ìˆ˜ê°€ 0ì´ë˜ì§€ ì•ŠëŠ”ë‹¤.
# ë‹¨, ë² íƒ€ ê³„ìˆ˜ ë²¡í„°ì˜ L2 Norm ê°’ì´ ì‘ì•„ì§.
# ë¦¿ì§€ë¥¼ ì ìš©í•˜ë©´ ì¢‹ì€ ì´ìœ : ë‹¤ì¤‘ê³µì„ ì„± ë•Œë¬¸ì— ê³„ìˆ˜ê°€ ì»¤ì§€ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆë‹¤.
# house price ë°ì´í„° ìµœì  ëŒë‹¤ ì°¾ì•„ë³´ê¸°


# ElasticNet
# ë¦¿ì§€ì™€ ë¼ì˜ë¥¼ í˜¼í•©í•œ ëª¨ë¸
# L1 Normê³¼ L2 Normì„ ëª¨ë‘ ì‚¬ìš©í•˜ì—¬ íŒ¨ë„í‹°ë¥¼ ë¶€ì—¬í•˜ëŠ” ë°©ë²•
from sklearn.linear_model import ElasticNet

elastic_model = ElasticNet(alpha=1.0, l1_ratio=0.5, max_iter=10000)  # alphaëŠ” íŒ¨ë„í‹° ê°€ì¤‘ì¹˜, l1_ratioëŠ” L1 Normê³¼ L2 Normì˜ ë¹„ìœ¨
elastic_model.fit(X, y)
elastic_model.coef_
elastic_model.intercept_


# CV ë¥¼ í†µí•œ ëŒë‹¤ ì°¾ê¸° - ë¼ì˜
import pandas as pd
import numpy as np
data_X = pd.read_csv("./QuickStartExample_x.csv")
y = pd.read_csv("./QuickStartExample_y.csv")


# ëŒë‹¤ 0.5ì¸ê²½ìš°
from sklearn.linear_model import Lasso

lasso_model = Lasso(alpha=0.5)  # alphaëŠ” íŒ¨ë„í‹° í•­ì˜ ê³„ìˆ˜ (ëŒë‹¤)
lasso_model.fit(data_X, y)
lasso_model.coef_  # íšŒê·€ê³„ìˆ˜
lasso_model.intercept_  # ì ˆí¸


# data_X ë¥¼ train / valid ë‚˜ëˆ„ê¸°
from sklearn.model_selection import train_test_split
X_train, X_valid, y_train, y_valid = train_test_split(data_X, 
                                                      y, 
                                                      test_size=0.3, 
                                                      random_state=2025)

# ì˜ˆì‹œ: ëŒë‹¤ 0.1ì”© ëŠ˜ë ¤ì„œ ì˜ˆì¸¡ ì„±ëŠ¥ í‰ê°€
for i in range(1, 6):
    lasso_model = Lasso(alpha=i / 10)  # alphaëŠ” íŒ¨ë„í‹° í•­ì˜ ê³„ìˆ˜ (ëŒë‹¤)
    lasso_model.fit(X_train, y_train)

    # Validation Set ì—ì„œì˜ ì„±ëŠ¥ í‰ê°€
    y_valid_hat = lasso_model.predict(X_valid)    # ì˜ˆì¸¡ yê°’

    # MSE ì–´ë–»ê²Œ ê³„ì‚°?
    print(f"MSE (lambda-{i / 10}): ", sum((y_valid_hat - y_valid['V1'])** 2))



# ìµœì  ëŒë‹¤ ì°¾ê¸°
alphas = np.linspace(0, 0.5, 1000)
valid_mse = []
for alpha in alphas:
    model = Lasso(alpha=alpha, max_iter=10000)
    model.fit(X_train, y_train)
    y_valid_hat = model.predict(X_valid)    # ì˜ˆì¸¡ yê°’
    valid_mse.append(sum((y_valid_hat - y_valid['V1'])** 2))

best_alpha = alphas[np.argmin(valid_mse)]
print(f"Best alpha(ëŒë‹¤): {best_alpha}")


import matplotlib.pyplot as plt

# ìµœì  ëŒë‹¤ ì‹œê°í™”
plt.figure(figsize=(8, 5))
plt.scatter(alphas, valid_mse, color='blue', label='Validation MSE')
plt.axvline(best_alpha, color='red', linestyle='--', label=f'Best Alpha = {best_alpha:.2f}')
plt.legend()
plt.xlabel('Alpha (ëŒë‹¤)')
plt.ylabel('MSE')
plt.show()


# ìµœì  ëŒë‹¤ë¥¼ í†µí•œ ë¼ì˜íšŒê·€
lasso_model = Lasso(alpha=best_alpha)
lasso_model.fit(X_train, y_train)
lasso_model.coef_  # íšŒê·€ê³„ìˆ˜
lasso_model.intercept_  # ì ˆí¸


# Cross Validation
from sklearn.linear_model import LassoCV
alphas = np.linspace(0, 0.5, 1000)
lasso_cv = LassoCV(alphas=alphas, 
                   cv=5,            # 5-fold cross-validation
                   max_iter=10000)


# ë°ì´í„° í•™ìŠµ (train set, valid set ë‚˜ëˆ„ì§€ ì•Šì•„ë„ë¨)
lasso_cv.fit(data_X, y)             
lasso_cv.alpha_      # ìµœì  ëŒë‹¤
lasso_cv.mse_path_.shape    # 1000, 5 (ëŒë‹¤ ê°œìˆ˜, fold ê°œìˆ˜)
lasso_cv.mse_path_[:, 0]    # ì²«ë²ˆì§¸ foldì˜ mse








'''
ê¸ˆìš”ì¼ í”„ë¡œì íŠ¸ ê´€ë ¨ ê³µì§€ ì…ë‹ˆë‹¤.

- ë°ì´í„°: Ames ë°ì´í„°(lon, lat ë²„ì „) + ì™¸ë¶€ë°ì´í„°(ììœ )
- ì£¼ì œ: Ames ë°ì´í„°ì™€ ê´€ë ¨í•œ ììœ ì£¼ì œ â€“ ì˜ˆì‹œëŠ” ì§€ë‚œ 3ê¸° ìë£Œ ì°¸ê³  í•  ê²ƒ.
- í˜•ì‹: ëŒ€ì‰¬ë³´ë“œ (ìŠ¤íƒœí‹±) + ì¸í„°ë ‰í‹°ë¸Œ ì‹œê°í™”

[í•„ìˆ˜ ìš”êµ¬ì‚¬í•­]
1. ë°ì´í„° íƒìƒ‰(EDA) ë° ì „ì²˜ë¦¬ ê²°ê³¼ ì‹œê°í™”
- ì£¼ìš” ë³€ìˆ˜ ë¶„í¬, ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì´ìƒì¹˜ íƒì§€ ë“±

2. ì§€ë„ ê¸°ë°˜ ì‹œê°í™”
- ì˜ˆ: Folium, Plotly ë“± ì‚¬ìš© ê°€ëŠ¥

3. ì¸í„°ë™í‹°ë¸Œ ìš”ì†Œ
- ì˜ˆ: Plotly ë“±

4. ëª¨ë¸ í•™ìŠµ í˜ì´ì§€
- íšŒê·€ëª¨ë¸ í›ˆë ¨ ê³¼ì •ê³¼ ê²°ê³¼ ì‹œê°í™”
- í˜ë„í‹° íšŒê·€ ëª¨ë¸ í•„ìˆ˜ ì‚¬ìš©

5. ìŠ¤í† ë¦¬í…”ë§ êµ¬ì„±
- ì „ì²´ ëŒ€ì‹œë³´ë“œê°€ í•˜ë‚˜ì˜ ë¶„ì„ íë¦„ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§ˆ ê²ƒ
- ê¼­ ì§‘ê°’ ì˜ˆì¸¡ì´ ì•„ë‹ˆì–´ë„ ë¨!

6. ì „ì²´ ë¶„ëŸ‰
- 4-5í˜ì´ì§€ë¡œ êµ¬ì„±


3ê¸° ìë£Œ ì°¸ê³ 
ì§€ë‚œ ê¸°ìˆ˜ Ames ê´€ë ¨ ë°œí‘œ ëŒ€ì‹œë³´ë“œ ì°¸ê³ ìš©

https://h-yoeunk.github.io/testdashboard/
https://yongraegod.github.io/Ames_Project/#
https://otacute.github.io/whaleshark/#
https://bonboneee.github.io/Project2/#
https://summerdklee.github.io/mywebsite/posts/team_proj_2/
https://ohseyn.github.io/00/#

'''