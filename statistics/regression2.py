plt.rcParams['font.family'] ='Malgun Gothic'
plt.rcParams['axes.unicode_minus'] =False

import pandas as pd
from sklearn.datasets import load_iris

iris = load_iris()
iris = pd.DataFrame(data=iris.data, columns=iris.feature_names)
iris.columns = ['Sepal_Length','Sepal_Width','Petal_Length','Petal_Width']


# íšŒê·€ë¶„ì„ ê¸°ë³¸ì½”ë“œ
from statsmodels.formula.api import ols
import statsmodels.api as sm

model = ols("Petal_Length ~ Petal_Width", data=iris).fit()
print(model.summary())


model = ols("Petal_Length ~ Petal_Width", data=iris).fit()
sm.stats.anova_lm(model)
#             df sum_sq mean_sq F PR(>F)
# Petal_Width 1.0 430.480647 430.480647 1882.452368 4.675004e-86
# Residual 148.0 33.844753 0.228681 NaN NaN
# ìœ ì˜ìˆ˜ì¤€ 5% í•˜ì—ì„œ F value (1882.5)ì™€ ëŒ€ì‘í•˜ëŠ” p-value ì„ ê³ ë ¤í•  ë•Œ, ë„ˆë¬´ ì‘ìœ¼ë¯€
# ë¡œ, ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•œë‹¤.



# ë‹¤ì¤‘íšŒê·€ë¶„ì„ ì‹¤í–‰
model2 = ols("Petal_Length ~ Petal_Width + Sepal_Length + Sepal_Width",
data=iris).fit()
print(model2.summary())


# ëª¨ë¸ì„ ì¶”ê°€í•˜ê±°ë‚˜ ë¹¼ê³ ì‹¶ì„ ë•Œ
# ê·€ë¬´ê°€ì„¤: Reduced Model (ë³€ìˆ˜ê°€ ì ì€ ëª¨ë¸) ì´ ì•Œë§ìŒ.
# ëŒ€ë¦½ê°€ì„¤: Full Model ì´ ì•Œë§ìŒ.
# Full model: Petal.Width + Sepal.Length + Sepal.Width
# Reduced model: Petal.Width
# F-ê²€ì •ì„ ì§„í–‰

model1 = ols('Petal_Length ~ Petal_Width', data=iris).fit() #mod1
model2 = ols('Petal_Length ~ Petal_Width + Sepal_Length + Sepal_Width',
data=iris).fit() #mod2
table = sm.stats.anova_lm(model1, model2) #anova
print(table)
# Full ëª¨ë¸ì´ ë‘ë²ˆì§¸ë¡œ ë“¤ì–´ê°€ì•¼ í•¨ì— ì£¼ì˜
# pval ì´ 0.05 ë¯¸ë§Œì´ë¯€ë¡œ ê·€ë¬´ê°€ì„¤ ê¸°ê°. (Full Model ì„ íƒ)



# 1ë³€ìˆ˜ ê·¸ë˜í”„: Histogram, Box plot
# ì£¼ìš” ì²´í¬ ì‚¬í•­ - ê° ë³€ìˆ˜ë“¤ ì¤‘ ë¶ˆê· í˜•í•œ ë¶„í¬ê°€ ì—†ëŠ”ì§€ í™•ì¸

# 2ë³€ìˆ˜ ê·¸ë˜í”„: Correlation plot
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
cols = ["Petal_Length", "Sepal_Length", "Sepal_Width", "Petal_Width"]
corr_mat = iris[cols].corr().round(2)
sns.heatmap(corr_mat, annot=True, cmap=plt.cm.Reds);
plt.show()


# ì”ì°¨ ê·¸ë˜í”„ì™€ ê²€ì •
# 1. ì •ê·œì„±
# Anderson-Darling Test or Shapiro-Wilk Test
# 2. ë“±ë¶„ì‚°ì„±
# F test(ë³€ìˆ˜ê°€ 2ê°œ), Levene, Bartlett

import scipy.stats as stats

# ì”ì°¨ ë½‘ì•„ì˜¤ê¸°
residuals = model2.resid
# ì”ì°¨ì˜ í•©ì€ 0 -> ì´ìœ ëŠ”?
# ì„ í˜• íšŒê·€ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ì˜¤ì°¨ ì œê³±í•©(Residual Sum of Squares) ì„ ìµœì†Œí™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„ë¨
sum(residuals)

fitted_values = model2.fittedvalues

plt.figure(figsize=(15,4))
plt.subplot(1,2,1)
plt.scatter(fitted_values, residuals)
plt.xlabel("residuals")
plt.ylabel("fitted value");

'''
Scatter plot
1. ì„ í˜•ì„±(linearity) ê°€ì • í™•ì¸
ì”ì°¨ë“¤ì´ ì˜ˆì¸¡ê°’(fitted values)ì— ëŒ€í•´ íŠ¹ì •í•œ íŒ¨í„´ ì—†ì´ í¼ì ¸ ìˆì–´ì•¼ í•´ìš”.

â†’ ì¦‰, ëœë¤í•˜ê²Œ í©ì–´ì§„ ëª¨ì–‘ì´ì–´ì•¼ í•´ìš” (êµ¬ë¦„ì²˜ëŸ¼ í¼ì§„ í˜•íƒœ).
ë§Œì•½ ê³¡ì„  í˜•íƒœë‚˜ Uì í˜•íƒœë¡œ ë³´ì¸ë‹¤ë©´:
ì„ í˜• íšŒê·€ëŠ” ì ì ˆí•˜ì§€ ì•Šë‹¤ëŠ” ì˜ë¯¸ì˜ˆìš”.
ë¹„ì„ í˜• ê´€ê³„ë¥¼ ì˜ì‹¬í•´ì•¼ í•©ë‹ˆë‹¤.

2. ë“±ë¶„ì‚°ì„±(Homoscedasticity) ê°€ì • í™•ì¸
ì”ì°¨ì˜ í¬ê¸°ê°€ ì˜ˆì¸¡ê°’ì´ ì»¤ì§ì— ë”°ë¼ ì ì  ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§€ëŠ” íŒ¨í„´ì´ ì—†ì–´ì•¼ í•´ìš”.

ë§Œì•½ ì”ì°¨ê°€ ê¹”ë•Œê¸° ëª¨ì–‘(ì‘ë‹¤ê°€ ì»¤ì§€ëŠ” ë“±)ì„ ë³´ì¸ë‹¤ë©´:

ë¶„ì‚°ì´ ì¼ì •í•˜ì§€ ì•Šë‹¤ëŠ” ëœ» (ì´ê±¸ ì´ë¶„ì‚°ì„±ì´ë¼ê³  í•´ìš”)

ì´ëŸ¬ë©´ íšŒê·€ ê²°ê³¼ì˜ ì‹ ë¢°ì„±ì´ ë–¨ì–´ì ¸ìš”.

'''



plt.subplot(1,2,2)
stats.probplot(residuals, plot=plt);
plt.show()




# Breuschâ€“Pagan / Cookâ€“Weisberg ê²€ì •
# ì•„ì´ë””ì–´: ì”ì°¨ê°€ ë“±ë¶„ì‚°ì„ ê°–ëŠ”ë‹¨ ì˜ë¯¸ëŠ” ë…ë¦½ë³€ìˆ˜ì— ì˜í•˜ì—¬ ì„¤ëª…ì´ ì•ˆëœë‹¤ëŠ” ëœ»

from statsmodels.stats.diagnostic import het_breuschpagan

model = ols('Petal_Length ~ Petal_Width + Sepal_Length + Sepal_Width', data=iris).fit()
bptest = het_breuschpagan(model.resid, model.model.exog)

# bptest: (LM stat, p-value, f-stat, f p-value)
print('BP-test statistics: ', bptest[0])
print("pval: ", bptest[1])
# BP-test statistics:  6.039114919618998
# pval:  0.10972262962330656
# H0: ëª¨ë“ ê³„ìˆ˜ê°€ 0ì´ë‹¤. (ì¦‰, ì”ì°¨ê°€ ë…ë¦½ë³€ìˆ˜ë“¤ê³¼ ë¬´ê´€í•˜ë‹¤)
# HA: 0ì´ ì•„ë‹Œ ê³„ìˆ˜ê°€ ì¡´ì¬í•œë‹¤. (ì”ì°¨ê°€ ì–´ë–¤ ë…ë¦½ë³€ìˆ˜ì™€ ê´€ë ¨ ìˆë‹¤.)
# ì¦‰, ê·€ë¬´ê°€ì„¤ì´ ê¸°ê°ë˜ë©´ ë“±ë¶„ì‚°ì„± ê°€ì •ì´ í‹€ë¦°ë‹¤.


# ì”ì°¨ ë…ë¦½ì„±: íŠ¹ì •íŒ¨í„´ì„ ë„ì§€ì•ŠëŠ”ì§€, ë¶„ì‚°ì´ ë³€í•˜ì§„ì•ŠëŠ”ì§€ ì²´í¬
# Durbin-Watson test ì‹¤ì‹œ
#  ê·€ë¬´ê°€ì„¤: ì”ì°¨ë“¤ê°„ì˜ ìƒê´€ì„±ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.
#  ëŒ€ë¦½ê°€ì„¤: ì”ì°¨ë“¤ê°„ì˜ ìê¸° ìƒê´€ì„±ì´ ì¡´ì¬í•œë‹¤
from statsmodels.stats.stattools import durbin_watson

dw_stat = durbin_watson(model2.resid)
print(dw_stat)
# dw_stat = 2.0 ì´ë©´ ì”ì°¨ê°€ ë…ë¦½ì ì´ë‹¤.
# dw_stat < 2.0 ì´ë©´ ì”ì°¨ê°€ ì–‘ì˜ ìê¸°ìƒê´€ê´€ê³„ê°€ ìˆë‹¤.
# dw_stat > 2.0 ì´ë©´ ì”ì°¨ê°€ ìŒì˜ ìê¸°ìƒê´€ê´€ê³„ê°€ ìˆë‹¤.
# 1.5 ~ 2.5 ì´ë©´ ì”ì°¨ê°€ ë…ë¦½ì ì´ë‹¤.




# ì—°ìŠµë¬¸ì œ

import pandas as pd
import numpy as np

url = "https://raw.githubusercontent.com/allisonhorst/palmerpenguins/master/inst/extdata/penguins.csv"
penguins = pd.read_csv(url)
print(penguins.head())

np.random.seed(2022)
train_index = np.random.choice(penguins.shape[0], 200)

# 1. train_indexë¥¼ ì‚¬ìš©í•´ì„œ í­ê·„ ë°ì´í„°ì—ì„œ ì¸ë±ìŠ¤ì— ëŒ€ì‘í•˜ëŠ” í‘œë³¸ë“¤ì„ ë½‘ì•„ì„œ 
# train_dataë¥¼ ë§Œë“œì„¸ìš”. (ë‹¨, ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ê²½ìš° ì œê±°)
train_data = penguins.iloc[train_index, :]
train_data.isna().sum()
train_data = train_data.dropna()



# 2. train_data ì˜ í­ê·„ ë¶€ë¦¬ê¸¸ì´ (bill_length_mm)ë¥¼ ë¶€ë¦¬ ê¹Šì´ (bill_depth_mm)ë¥¼
#  ì‚¬ìš©í•˜ì—¬ ì‚°ì ë„ë¥¼ ê·¸ë ¤ë³´ì„¸ìš”.
sns.scatterplot(x=train_data['bill_depth_mm'], y=train_data['bill_length_mm'])

# 3. í­ê·„ ë¶€ë¦¬ê¸¸ì´ (bill_length_mm)ë¥¼ ë¶€ë¦¬ ê¹Šì´ (bill_depth_mm)ì˜ ìƒê´€ê³„ìˆ˜ë¥¼ êµ¬í•˜ê³ , 
# ë‘ ë³€ìˆ˜ ì‚¬ì´ì— ìœ ì˜ë¯¸í•œ ìƒê´€ì„±ì´ ì¡´ì¬í•˜ëŠ”ì§€ ê²€ì •í•´ë³´ì„¸ìš”.
model = ols('bill_length_mm ~ bill_depth_mm', data=train_data).fit
# ê·€ë¬´ê°€ì„¤: ë‘ ë³€ìˆ˜ ì‚¬ì´ì— ìƒê´€ì„±ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.
# ëŒ€ë¦½ê°€ì„¤: ë‘ ë³€ìˆ˜ ì‚¬ì´ì— ìƒê´€ì„±ì´ ì¡´ì¬í•œë‹¤. 
from scipy.stats import pearsonr
corr_coef, p_value = pearsonr(train_data['bill_length_mm'],
                              train_data['bill_depth_mm'])
print(f"ìƒê´€ê³„ìˆ˜: {corr_coef:.3f}")
print(p_value)
# pvalue ê°€ ë‚®ìœ¼ë¯€ë¡œ O


# 4. í­ê·„ ë¶€ë¦¬ê¸¸ì´ (bill_length_mm)ë¥¼ ë¶€ë¦¬ ê¹Šì´ (bill_depth_mm)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¤ëª…í•˜ëŠ” 
# íšŒê·€ ëª¨ë¸ì„ ì í•©ì‹œí‚¨ í›„ 2ë²ˆì˜ ì‚°ì ë„ì— íšŒê·€ ì§ì„ ì„ ë‚˜íƒ€ë‚´ ë³´ì„¸ìš”. (ëª¨ë¸ 1)
sns.scatterplot(data=train_data,
                x='bill_depth_mm', y='bill_length_mm',
                edgecolor='w', s=50)
x_values = train_data['bill_depth_mm']
y_values = 55.4110 - 0.7062 * x_values
plt.plot(x_values, y_values, 
         color='red', label='Regression Line')
plt.grid(True)
plt.legend()
plt.show()


# 5. ì í•©ëœ íšŒê·€ ëª¨ë¸ì´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œì§€ íŒë‹¨í•´ë³´ì„¸ìš”.
print(model.summary())
# ìœ ì˜ìˆ˜ì¤€ 5%í•˜ì—ì„œ F ê²€ì • í†µê³„ëŸ‰ ê°’ 12.93ì— ëŒ€ì‘í•˜ëŠ” pâ€valueê°’ 0.000409ì— ë¹„ì¶”ì–´ ë³´ì•˜ì„ ë•Œ, 
# íšŒê·€ ëª¨ë¸ì€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê²ƒìœ¼ë¡œ íŒë‹¨í•œë‹¤.


# ì”ì°¨ ë“±ë¶„ì‚°ì„± ê²€ì • (bptest)
bptest = het_breuschpagan(model.resid, model.model.exog)
bptest  # (LM stat, p-value, f-stat, f p-value)
pval = bptest[1]
pval < 0.05     
# H0: ëª¨ë“ ê³„ìˆ˜ê°€ 0ì´ë‹¤. (ì¦‰, ì”ì°¨ê°€ ë…ë¦½ë³€ìˆ˜ë“¤ê³¼ ë¬´ê´€í•˜ë‹¤)
# HA: 0ì´ ì•„ë‹Œ ê³„ìˆ˜ê°€ ì¡´ì¬í•œë‹¤. (ì”ì°¨ê°€ ì–´ë–¤ ë…ë¦½ë³€ìˆ˜ì™€ ê´€ë ¨ ìˆë‹¤.)


# 6. ğ‘… ê°’ì„ êµ¬í•œ í›„ ì˜ë¯¸ë¥¼ í•´ì„í•´ ë³´ì„¸ìš”
# 0.062
# ë„ˆë¬´ì‘ë‹¤. ë¶€ë¦¬ ê¹Šì´ë¡œ ë¶€ë¦¬ ê¸¸ì´ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ì„¤ëª…ë ¥ì´ 6.2% ë°–ì— ë˜ì§€ì•ŠëŠ”ë‹¤.
# ì¶”ê°€ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì˜ ì„¤ëª…ë ¥ì„ ë†’ì´ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì.


# 7. ì í•©ëœ íšŒê·€ ëª¨ë¸ì˜ ê³„ìˆ˜ë¥¼ í•´ì„í•´ ë³´ì„¸ìš”.
# ì ˆí¸ê³¼ ê¸°ìš¸ê¸°ê°€ ëª¨ë‘ ìœ ì˜í•˜ê²Œ ë‚˜ì˜¤ì§€ë§Œ, ê° ë³€ìˆ˜ì˜ ëœ»ì„ ê³ ë ¤í•˜ë©´, ì ˆí¸ì˜ í•´ì„ì€ ë¬´ì˜ë¯¸í•˜ë‹¤. 
#   (ë¶€ë¦¬ ê¹Šì´ 0ì¸ ê²½ìš°, ë¶€ë¦¬ ê¸¸ì´ 56 mm)

# ê¸°ìš¸ê¸°â€0.7062 ê°’ì˜ ì˜ë¯¸ëŠ”, íŒ”ë¨¸ í­ê·„ì˜ ê²½ìš° ë¶€ë¦¬ ê¹Šì´ê°€ 1mm ì¦ê°€ í•  ë•Œ, 
# ë¶€ë¦¬ ê¸¸ì´ëŠ” í‰ê· ì ìœ¼ë¡œ 0.7062 mm ë§Œí¼ ê°ì†Œí•˜ëŠ” ê²½í–¥ì„ ë³´ì¸ë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆë‹¤.



# 8. 
# 1ë²ˆì—ì„œ ì í•©í•œ íšŒê·€ ëª¨ë¸ì— ìƒˆë¡œìš´ ë³€ìˆ˜ (ì¢… - species) ë³€ìˆ˜ë¥¼ ì¶”ê°€í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ì„±ë³„ ë³€ìˆ˜ ì •
# ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì  ìƒ‰ê¹”ì„ ë‹¤ë¥´ê²Œ ì‹œê°í™” í•œ í›„ ì í•©ëœ ëª¨ë¸ì˜ íšŒê·€ ì§ì„ ì„ ì‹œê°í™” í•´ë³´ì„¸ìš”. (ëª¨ë¸
# 2)

model2 = ols('bill_length_mm ~ bill_depth_mm + species', data=train_data).fit()
print(model2.summary())

train_data['fitted'] = model2.fittedvalues
# ì‚°ì ë„ (ì‹¤ì œ ë°ì´í„°)
sns.scatterplot(data=train_data,
                x='bill_depth_mm', y='bill_length_mm',
                hue='species', palette='deep', edgecolor='w', s=50)

# ê·¸ë£¹ë³„(facetë³„)ë¡œ fitted ì„  ê·¸ë¦¬ê¸°
for species, df in train_data.groupby('species'):
    df_sorted = df.sort_values('bill_depth_mm')  # Xì¶• ê¸°ì¤€ ì •ë ¬
    sns.lineplot(data=df_sorted,
                 x='bill_depth_mm', y='fitted', color="red")
plt.title("Regression Lines(fitted)")
plt.legend()
plt.show()




# 9. ì¢… ë³€ìˆ˜ê°€ ìƒˆë¡œ ì¶”ê°€ëœ ëª¨ë¸ 2ê°€ ëª¨ë¸ 1 ë³´ë‹¤ ë” ì¢‹ì€ ëª¨ë¸ì´ë¼ëŠ” ê·¼ê±°ë¥¼ ì œì‹œí•˜ì„¸ìš”.
sm.stats.anova_lm(model, model2)
# p_value ê°€ ì‘ë‹¤. (ê·€ë¬´ê°€ì„¤ ê¸°ê°, Full model ì„ íƒ)


# 10. ëª¨ë¸ 2ì˜ ê³„ìˆ˜ì— ëŒ€í•œ ê²€ì •ê³¼ ê·¸ ì˜ë¯¸ë¥¼ í•´ì„í•´ ë³´ì„¸ìš”.
print(model2.summary())


# 11. ëª¨ë¸ 2 ì— ì”ì°¨ ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ê³ , íšŒê·€ëª¨ë¸ ê°€ì •ì„ ë§Œì¡±í•˜ëŠ”ì§€ ê²€ì¦ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.scatter(model2.fittedvalues, model2.resid)
# ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ì— í´ëŸ¬ìŠ¤í„°ì²˜ëŸ¼ ëª¨ì—¬ ìˆìŒ

plt.subplot(1, 2, 2)
stats.probplot(model2.resid, plot=plt)
plt.show()

bptest = het_breuschpagan(model2.resid, model2.model.exog)
if bptest[1] < 0.05:
    print("bptest ì—ì„œ pvalue ê°’ì´ 0.05ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤. ë“±ë¶„ì‚°ì„±ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
else:
    print("pvalue ê°’ì´ 0.05ë³´ë‹¤ í½ë‹ˆë‹¤. ë“±ë¶„ì‚°ì„±ì„ ë§Œì¡±í•©ë‹ˆë‹¤.")

from scipy.stats import anderson
# ê·€ë¬´ê°€ì„¤: ë°ì´í„°ëŠ” ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤. / ëŒ€ë¦½ê°€ì„¤: ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ì•ŠëŠ”ë‹¤.
anderson(model2.resid, dist='norm')
# í†µê³„ëŸ‰ì´ ì„ê³„ê°’ë³´ë‹¤ ì‘ìœ¼ë©´ ë§Œì¡±. (ê¸°ê° ëª» í•¨)
# ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. (ê¸°ê° ëª» í•¨)






'''

ì—°ìŠµë¬¸ì œ ëª¨ìŒ

'''
import numpy as np
from scipy.stats import pearsonr
x = np.array([1, 2, 3, 4, 5])
y = np.array([2, 4, 6, 8, 13])
# í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ì™€ p-value ê³„ì‚°
corr_coef, p_value = pearsonr(x, y)
print(f"ìƒê´€ê³„ìˆ˜: {corr_coef:.3f}")
print(f"p-value: {p_value:.3f}")

# p-value ê°€ 0.05ë³´ë‹¤ ì‘ìœ¼ë¯€ë¡œ ê·€ë¬´ê°€ì„¤ ê¸°ê°. ìƒê´€ê´€ê³„ê°€ ìˆë‹¤.
# 0.974 ë§Œí¼ ìƒê´€ê´€ê³„ê°€ ìˆë‹¤. (ê°•í•œ ìƒê´€ê´€ê³„)



# 2. 
x = np.array([1, 2, 3, 4, 10, 11, 12])
y = np.array([2, 4, 6, 8, 100, 200, -100])

# ì´ìƒì¹˜ê°€ í¬í•¨ëœ ê²½ìš°
corr_coef, p_val = pearsonr(x, y)
print(f"ìƒê´€ê³„ìˆ˜ (ì´ìƒì¹˜ í¬í•¨): {corr_coef:.3f}")
print(f"p-value (ì´ìƒì¹˜ í¬í•¨): {p_val:.3f}")
# ê¸°ê°. ìƒê´€ê´€ê³„ê°€ ì—†ë‹¤.

# ì´ìƒì¹˜ë¥¼ ì œì™¸í•œ ê²½ìš°
x_no_outliers = np.array([1, 2, 3, 4])
y_no_outliers = np.array([2, 4, 6, 8])
corr_coef, p_val = pearsonr(x_no_outliers, y_no_outliers)
print(f"ìƒê´€ê³„ìˆ˜ (ì´ìƒì¹˜ ì œì™¸): {corr_coef:.3f}")
print(f"p-value (ì´ìƒì¹˜ ì œì™¸): {p_val:.3f}")
# ê°•í•œ ì–‘ì˜ ìƒê´€ê´€ê³„ê°€ ìˆìŠµë‹ˆë‹¤.

# ì´ìƒì¹˜ê°€ ìƒê´€ê³„ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥
print("ì´ìƒì¹˜ê°€ ìƒê´€ê³„ìˆ˜ì— í° ì˜í–¥ì„ ë¯¸ì³ ì™œê³¡ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


# 3.
x = np.array([1, 2, 3, 4, 5])
y = np.array([3, 6, 9, 12, 15])

# ë‹¨ìˆœ ì„ í˜•íšŒê·€ ê³„ìˆ˜ B0, B1ì„ ì§ì ‘ ê³„ì‚°í•˜ì‹œì˜¤.
# íšŒê·€ì§ì„  ë°©ì •ì‹ì„ êµ¬í•˜ì‹œì˜¤.
B1 = np.cov(x, y, ddof=1)[0, 1] / np.var(x, ddof=1)
B0 = np.mean(y) - B1 * np.mean(x)

print(f"B0: {B0:.2f}")
print(f"B1: {B1:.2f}")
print(f"íšŒê·€ì§ì„  ë°©ì •ì‹: y = {B0:.2f} + {B1:.2f}x")




# 4.
x = np.array([1, 2, 3, 4, 5])
y = np.array([3, 5, 7, 9, 11])
# ìƒê´€ê³„ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ B1_hatì„ ê³„ì‚°í•˜ì‹œì˜¤.
# B1_hat = r * (sy / sx)
B1_hat = pearsonr(x, y)[0] * (np.std(y, ddof=1) / np.std(x, ddof=1))
print(f"B1_hat: {B1_hat:.2f}")
print("ì§ì ‘ ë¹„êµí•œ ê²°ê³¼ì™€ ê°™ë‹¤.")


# 5. 
import pandas as pd
from statsmodels.formula.api import ols
from sklearn.datasets import fetch_california_housing
cal = fetch_california_housing(as_frame=True)
df = cal.frame
# MedHouseValì„ ì¢…ì†ë³€ìˆ˜ë¡œ í•˜ê³ , AveRooms, AveOccupì„ ë…ë¦½ë³€ìˆ˜ë¡œ ì„¤ì •í•œ ì„ í˜•íšŒê·€ëª¨í˜•ì„ ì í•©í•˜ì‹œì˜¤.
# ëª¨ë¸ì˜ íšŒê·€ì‹ì„ êµ¬í•˜ì‹œì˜¤.
model = ols('MedHouseVal ~ AveRooms + AveOccup', data=df).fit()
print(model.summary())
# íšŒê·€ì‹: MedHouseVal = -0.0708 * AveRooms + -0.0026 * AveOccup + 1.6919

# p-value ã…”ë“¤ì€ AveOccup ë§Œ 0.001 ì´ê³ , ë‚˜ë¨¸ì§€ëŠ” ë‹¤ 0.000 ì´ë¯€ë¡œ ìœ ì˜ë¯¸í•˜ë‹¤.


# 6.
model = ols('MedHouseVal ~ AveRooms + AveOccup', data=df).fit()
# t-value ê°€ ê°€ì¥ í° ë³€ìˆ˜ëŠ” ë¬´ì—‡ì¸ê°€?
# í•´ë‹¹ ë³€ìˆ˜ì˜ p-value ëŠ” ì–¼ë§ˆì¸ê°€?
# tsatistic ê°€ ê°€ì¥ í° ë³€ìˆ˜ëŠ” AveRooms ì´ê³ , p-value ëŠ” 0.000 ì´ë‹¤.


# 7.
df['IncomeLevel'] = pd.qcut(df['MedInc'], q=3, labels=['Low', 'Mid', 'High'])
model = ols('MedHouseVal ~ AveRooms + AveOccup + C(IncomeLevel)', data=df).fit()
# ë²”ì£¼í˜• ë³€ìˆ˜ IncomeLevelì—ì„œ ê°€ì¥ ìœ ì˜ë¯¸í•œ ë”ë¯¸ ë³€ìˆ˜ëŠ” ë¬´ì—‡ì¸ê°€?
print(model.summary())  # C(IncomeLevel)[T.High] , 1.6558
# í•´ë‹¹ ë³€ìˆ˜ì˜ íšŒê·€ ê³„ìˆ˜ë¥¼ êµ¬í•˜ì„¸ìš”.



# 8.
from statsmodels.stats.stattools import durbin_watson
dw_stat = durbin_watson(model.resid)
print(dw_stat)
# dw_stat = 2.0 ì´ë©´ ì”ì°¨ê°€ ë…ë¦½ì ì´ë‹¤.
# dw_stat < 2.0 ì´ë©´ ì”ì°¨ê°€ ì–‘ì˜ ìê¸°ìƒê´€ê´€ê³„ê°€ ìˆë‹¤.
# dw_stat > 2.0 ì´ë©´ ì”ì°¨ê°€ ìŒì˜ ìê¸°ìƒê´€ê´€ê³„ê°€ ìˆë‹¤.

# dw_stat ì´ 0.6ì¸ê²½ìš°
# ì”ì°¨ê°€ ì–‘ì˜ ìê¸°ìƒê´€ê´€ê³„ê°€ ìˆë‹¤.
# ìê¸° ìƒê´€ì´ ìˆì„ ê²½ìš°, íšŒê·€ëª¨í˜•ì˜ ì‹ ë¢°ì„±ì´ ë–¨ì–´ì§„ë‹¤.


# 9.
from statsmodels.stats.diagnostic import het_breuschpagan
bp_test = het_breuschpagan(model.resid, model.model.exog)
print(bp_test)  # (LM stat, p-value, f-stat, f p-value)
bp_test[1] < 0.05
# ë“±ë¶„ì‚°ì„± ìœ„ë°°
# ìœ„ë°°ë˜ë©´, íšŒê·€ëª¨í˜•ì˜ ì‹ ë¢°ì„±ì´ ë–¨ì–´ì§„ë‹¤.



# 10.
from statsmodels.stats.outliers_influence \
import variance_inflation_factor
X = df[['AveRooms', 'AveOccup', 'HouseAge']]
vif_df = pd.DataFrame()
vif_df['Variable'] = X.columns
# ê³„ì‚°

# .vifê°€ 